相关链接：
https://radimrehurek.com/gensim/models/word2vec.html
https://zhuanlan.zhihu.com/p/26306795/
https://zhuanlan.zhihu.com/p/27234078
https://blog.csdn.net/xpy870663266/article/details/101849044
https://blog.csdn.net/ling620/article/details/99441942?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link

embed原意是嵌入，embedding在word2vec中意义如下：
简单地说，embedding就是把一个东西映射到一个向量 x。如果两个东西很像，那么得到的向量  x1 和 x2 的欧式距离很小。
例一：Word Embedding，把单词 w 映射到向量 x。如果两个词的原意接近，比如coronavirus和covid，那么它们映射后得到的两个词向量 x1 和 x2 的欧式距离很小。
例二：User Embedding，把用户 ID 映射到向量 x。推荐系统中需要用一个向量表示一个用户。如果两个用户的行为习惯接近，那么他们对应的向量  x1 和 x2 的欧式距离很小。
例三：Graph Embedding，把图中的每个节点映射成一个向量 x。如果图中两个节点接近，比如它们的最短路很小，那么它们embed得到的向量 x1 和 x2 的欧式距离很小。

样例代码

模型初始化，注意模型是流式训练，不必读取所有训练语料至ram中，可以保存结果后继续训练。
``` python
>>>from gensim.test.utils import common_texts
>>>from gensim.models import Word2Vec
>>>model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)
>>>model.save("word2vec.model")


>>>model = Word2Vec.load("word2vec.model")
>>>model.train([["hello", "world"]], total_examples=1, epochs=1)
(0, 2)
```

model对象与词向量对象的关系：
model对象包含完整的权重、词频、二叉树结构，而词向量对象只包含词键和向量，因此无法继续训练模型，但相应地，其体积非常小。

从已有的库中可以加载词向量对象，但同上所述，得到的结果无法继续训练。
``` python
from gensim.test.utils import datapath

# Load a word2vec model stored in the C *text* format.
wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)
# Load a word2vec model stored in the C *binary* format.
wv_from_bin = KeyedVectors.load_word2vec_format(datapath("euclidean_vectors.bin"), binary=True)
```
