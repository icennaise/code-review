参考链接：
https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html


你可能已经熟悉了Vector部分的bag-of-words模型。这个模型将每个文档转换为一个固定长度的整数向量。例如，给定句子。

约翰喜欢看电影。玛丽也喜欢看电影。
约翰也喜欢看足球比赛。玛丽讨厌足球。

该模型输出矢量。

[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]
[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]

每个向量有10个元素，每个元素计算一个特定单词在文档中出现的次数。元素的顺序是任意的。在上面的例子中，元素的顺序对应于这些词。["约翰"，"喜欢"，"看"，"电影"，"玛丽"，"也"，"足球"，"游戏"，"憎恨"]。
词袋模型出乎意料地有效，但也有几个弱点。
首先，它们失去了所有关于词序的信息。"约翰喜欢玛丽 "和 "玛丽喜欢约翰 "对应于相同的向量。有一个解决方案：n-grams袋模型考虑长度为n的单词短语，将文档表示为固定长度的向量，以捕捉局部的词序，但受到数据稀少和高维度的影响。
其次，该模型并不试图学习底层单词的意义，因此，向量之间的距离并不总是反映意义的差异。Word2Vec模型解决了这第二个问题。
Word2Vec是一个较新的模型，它使用浅层神经网络将单词嵌入到一个较低维度的向量空间。其结果是一组词向量，在向量空间中靠近的向量根据上下文具有相似的含义，而相互远离的词向量具有不同的含义。例如，strong和powerful会很接近，而strong和Paris则相对较远。

这个模型有两个版本，Word2Vec类都实现了它们。
跳格(SG)
连续词包(CBOW)
